View(gdp)
j<-gdp$Economy
j
grep("^United",j)
grepl("^United",j)
table(grepl("^United",j))
edu <- read.csv("~/Documents/r_code/getdata-data-EDSTATS_Country.csv", stringsAsFactors=FALSE)
View(edu)
names(edu)[1]="Country"
ans<-merge(gdp,edu,by="Country")
grep("june",ans)
grep("june",edu)
grep("june",gdp)
grep("June",gdp)
grep("June",edu)
grep("June",ans)
library(quantmod)
install.packages(quantmod)
install.packages("quantmod")
library(quantmod)
amzn=getSymbols("AMZN",auto.assign=FALSE)
sampleTimes=index(amzn)
View(amzn)
n<-amzn$row.names
amzn$row.names
amzn
kip<-amzn
kip
index(amzn)
times<-index(amzn)
times
times[1]
grep("2012",times)
grepl("2012",times)
table(grepl("2012",times))
install.packages("twitteR")
install.packages("ROAuth")
install.packages("wordcloud")
install.packages("tm")
library(twitteR)
getTwitterOAuth(TPiP1y67fxUEukHrtuRVN2vB3,TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv)
getTwitterOAuth("TPiP1y67fxUEukHrtuRVN2vB3","TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv")
getTwitterOAuth("TPiP1y67fxUEukHrtuRVN2vB3","TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv")
getTwitterOAuth("TPiP1y67fxUEukHrtuRVN2vB3","TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv")
getTwitterOAuth("TPiP1y67fxUEukHrtuRVN2vB3","TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv")
getTwitterOAuth("TPiP1y67fxUEukHrtuRVN2vB3","TnbXTx8qNetUYtoTSiBY8yoy00NUU4PkwgqQl5rwSbidtFDHPv")
source('~/Documents/r_code/twitter.R', echo=TRUE)
registerTwitterOAuth(twitCred)
a<-searchTwitter('Tony Abbot',geocode='-31.946336,115.851806,50km',n=5000, retryOnRateLimit=1)
a<-searchTwitter('Tony Abbot',geocode='-31.946336,115.851806,50km',n=15, retryOnRateLimit=1)
a<-searchTwitter('Tony Abbot',geocode='-31.946336,115.851806,50km',n=13, retryOnRateLimit=1)
a
a<-searchTwitter('Tony Abbot',geocode='-31.946336,115.851806,50km',n=5000, retryOnRateLimit=1)
a<-searchTwitter('Tony Abbot',geocode='-31.946336,115.851806,50km',n=5000)
a<-searchTwitter('shirtfront',geocode='-31.946336,115.851806,50km',n=5000)
a
a
text<-sapply(a,function(x) x$getText())
text
text_corpus<-Corpus(VectorSource(text))
library(tm)
text_corpus<-Corpus(VectorSource(text))
r_map<-tm_map(text_corpus,tolower)
r_map<-tm_map(text_corpus,removePunctuation)
r_map<-tm_map(text_corpus,function(x)removeWords(x,stopwords()))
wordcloud(r_map)
library(wordcloud)
wordcloud(r_map)
tw_df<-twListToDF(a)
tw_df
View(tw_df)
order(tw_df,tw_df$favoriteCount)
order(tw_df,favoriteCount)
tw_df[order(tw_df$favoriteCount),]
kip<-tw_df[order(tw_df$favoriteCount),]
head(kip)
kip<-tw_df[order(-tw_df$favoriteCount),]
head(kip)
source('~/Documents/r_code/twitter.R', echo=TRUE)
source('~/Documents/r_code/twitter.R', echo=TRUE)
r_stats<- searchTwitter("#abbott", n=1500)
r_stats<-searchTwitter("#abbott", n=1500)
library(OAuth)
library("ROAuth")
r_stats<-searchTwitter("#abbott", n=1500)
r_stats<-searchTwitter("abbott", n=1500)
twitCred
twitCred$handshake()
save(twitCred,file="twitter auth.Rdata")
registerTwitterOAuth(twitCred)
perth<-searchTwitter('abbott', geocode='-31.953073,115.857693,50km', n=1500)
perth_df<-twListToDF(perth)
View(perth_df)
table(perthdf[c(3,11)])
table(perth_df[c(3,11)])
table(perth_df[c(11,3)])
perth_df[perth_df$retweetCount>12,]
perth_df[,perth_df$retweetCount>12]
gt<-subset(perth_df,perth_df$retweetCount>24)
gt
table(gt[c(11,3)])
gt<-subset(perth_df,perth_df$favoriteCount>24)
gt<-subset(perth_df,perth_df$favoriteCount>12)
gt<-subset(perth_df,perth_df$favoriteCount>5)
table(gt[c(11,3)])
gt[gt$screenName=="linessue",]
?load
save(perth,file="perthtwit.R")
save(perth,file="perthtwit.Rdata")
melb<-searchTwitter('abbott', geocode='-37.807071,144.939773,50km', n=1500)
melb_df<-twListToDF(melb)
View(melb_df)
length(melb_df[melb_df$favoriteCount>20])
length(melb_df[melb_df$favoriteCount>20,])
length(melb_df[melb_df$favoriteCount>10,])
length(melb_df[melb_df$favoriteCount>5,])
dims(melb_df[melb_df$favoriteCount>5,])
dim(melb_df[melb_df$favoriteCount>5,])
dim(melb_df[melb_df$favoriteCount>2,])
dim(melb_df[melb_df$retweetCount>2,])
dim(melb_df[melb_df$retweetCount>30,])
dim(melb_df[melb_df$retweetCount>50,])
dim(melb_df[melb_df$retweetCount>80,])
kip<-melb_df[melb_df$retweetCount>80,]
table(kip[c(11,3)])
table(kip[c(11,12)])
abbot_brittney<-melb_df[melb_df$screenName=="abbott_brittney",]
View(abbot_brittney)
abbot_brittney<-melb_df[melb_df$screenName=="ALLANPHC",]
View(abbot_brittney)
setwd("~/Documents/r_code/Presidential-Semantic-Analysis")
setwd("~/Documents/r_code/Presidential-Semantic-Analysis")
hip<-president()
source('~/Documents/r_code/Presidential-Semantic-Analysis/presidentialsemantics.R', echo=TRUE)
install.packages("ggplot2")
source('~/Documents/r_code/Presidential-Semantic-Analysis/presidentialsemantics.R', echo=TRUE)
source('~/Documents/r_code/Presidential-Semantic-Analysis/presidentialsemantics.R', echo=TRUE)
ans<-president()
source('~/Documents/r_code/Presidential-Semantic-Analysis/ScoreSentiment.R', echo=TRUE)
ans<-president()
positive.words <- scan("~/opinion-lexicon-English/positive-words.txt", what='character',comment.char=';')
positive.words <- scan("opinion-lexicon-English/positive-words.txt", what='character',comment.char=';')
negative.words <- scan("opinion-lexicon-English/negative-words.txt", what='character',comment.char=';')
ans<-president()
ans
View(ans)
textPerth=laply(perth, function(t) t$getText())
resultPerth=score.sentiment(textPerth, positive.words, negative.words)
View(resultPerth)
sum(resultPerth>0)
sum(resultPerth$score>0)
sum(resultPerth$score<0)
sum(resultPerth$score==0)
setwd("~/Documents/r_code")
install.packages(pkgs="sentiment_0.2.tar.gz", type="source", repos=NULL)
install.packages("Rstem")
install.packages("Rstem", repos = "http://www.omegahat.org/R")
library(qdap)
install.packages("qdap")
save(melb,file="melbtwit.Rdata")
sum(resultPerth$score<0)
sum(resultPerth$score<0)
sum(resultPerth$score==0)
sum(resultPerth$score>0)
textMelb=laply(melb, function(t) t$getText())
resultMelb=score.sentiment(textMelb, positive.words, negative.words)
sum(resultMelb$score>0)
sum(resultMelb$score<0)
sum(resultMelb$score==0)
dt<-load("melbtwit.R")
?load
load("melbtwit.Rdata",.GlobalEnv)
df<-twListToDF(melb)
library(twitteR)
df<-twListToDF(melb)
df[1,1]
library(qdap)
x<-df[1,1]
x
polarity(x)
View(df)
x<-polarity(df$text)
gsub("[^[:alnum:]///' ]", "", df)
df2<-gsub("[^[:alnum:]///' ]", "", df)
df2<-gsub("[^[:alnum:]///' ]", "", df$text)
df2
df2[1]
x<-polarity(df2)
x
vapply
?vapply
x<-vapply(df2,polarity)
x<-lapply(df2,polarity)
head(x)
?polarity
polarity(df2,grouping.var=1:1500)
x<-polarity(df2,grouping.var=1:1500)
head(x)
x[1,1]
x[1]
df2[1:10]
hip<-df2[1:10]
hip
polarity(hip)
polarity(hip,grouping=1:10)
x<-polarity(df2,grouping=1:1500)
heaed(x)
head(X)
head(x)
hip<-df2[1:500]
x<-polarity(hip,grouping=1:500)
x
x$500
x[1,1]
x[1,2]
x[1,3]
hip<-df2[1:10]
hip
polarity(hip,grouping=1:10)
x<-polarity(hip,grouping=1:10)
dim(x)
x<-polarity(hip,grouping.var=1:10)
x
dim(x)
y<-1:10
cbind(y,hip)
hik<-cbind(y,hip)
y<-as.numeric(1:10)
hik<-cbind(y,hip)
hik[1,1]
hik[1,2]
x<-polarity(hip,grouping.var=y)
x
dim(x)
x$y
polarity(hip,grouping.var=y)
polarity(hip,grouping.var=all)
polarity(hip,all)
polarity(hip)$all
polarity(hip)$group
polarity(hip)$digits
polarity(hip)$all
df<-polarity(hip)$all
df
df$polarity
hip<-polarity(df2)$all
sum(hip$polarity>0)
sum(hip$polarity<0)
sum(hip$polarity==0)
load('perthtwit.Rdata')
pdf<-twListToDF(perth)
perth_sent<-polarity(pdf$text)$all
perth_sent<-polarity(gsub("[^[:alnum:]///' ]", "",pdf$text)$all
)
perth_sent<-polarity(gsub("[^[:alnum:]///' ]", "",pdf$text))$all
sum(perth_sent$polarity==0)
sum(perth_sent$polarity<0)
sum(perth_sent$polarity>0)
head(perth_sent)
perth_sent$neg.words
perth_text<-gsub("[^[:alnum:]///' ]", "",pdf$text)
perth_text<-gsub("-", "",pdf$text)
perth_sent<-polarity(perth_sent)$all
head(perth_sent)
perth_sent$pos.words
perth_sent$neg.words
sum(perth_sent$polarity<0)
sum(perth_sent$polarity>0)
pol<-perth_sent$polarity
head(pol)
head(pdf)
head(pdf$text)
perth_text<-pdf$text
pertht2<-gsub("[^[:alnum:]///' ]", "",perth_text)
head(pertht2)
grep(pertht2,"-")
?grep
grep("-",pertht2)
grepl("-",pertht2)
kip<-polarity(pertht2)$all
head(kip)
pos<-kip$pos.words
pos
grep("-",pos)
grepl("-",pos)
pos[grepl("-",pos)]
pos[!grepl("-",pos)]
View(hip)
View(hip)
sum(hip$polarity<0)
sum(hip$polarity>0)
sum(hip$polarity==0)
head(kip)
negwords<-kip$neg.words
negwords<-negwords[!grepl("-",negwords)]
negwords
library(wordcloud)
wordcloud(negwords)
?wordcloud
wordcloud(perth_text)
negwords
wordcorp<-Corpus(negwords)
Corpus(negwords)
?Corpus
Corpus(perth_text)
negwords<-sapply(negwords, function(x) x$getText())
negwords<-sapply(negwords, function(x) getText())
library(tm)
?getText
unlist(negwords)
Corpus(unlist(negwords))
Corpus(VectorSource(unlist(negwords)))
corp<-Corpus(VectorSource(unlist(negwords)))
wordcloud(corp)
neg
neg<-hip$polarity<0
pos<-hip$polarity>0
sum(neg)
mean(neg)
mean(pos)
dim(neg)
neg
pos<-hip[hip$polarity>0]
pos<-hip[hip$polarity>0,]
pos<-subset(hip, polarity>0)
neg<-subset(hip, polarity<0)
head(pos)
sum(pos$polarity)
sum(neg$polarity)
hip[hip$polarity<0]
hip[hip$polarity<0,]
hip$polarity[hip$polarity<0,]
hip$polarity[hip$polarity<0]
load("melbtwit.Rdata")
source('~/Documents/r_code/poly_analysis.R', echo=TRUE)
wordcloud(melb_corp)
m2<-searchTwitter('abbott', geocode='-37.807071,144.939773,50km', n=10)
load("twitter auth.Rdata")
registerTwitterOAuth(cred)
registerTwitterOAuth(twitCred)
m2<-searchTwitter('abbott', geocode='-37.807071,144.939773,50km', n=10)
m2
m2<-searchTwitter('#abbott', geocode='-37.807071,144.939773,50km', n=10)
twListToDF(m2)
m2<-searchTwitter('#Auspol', geocode='-37.807071,144.939773,50km', n=10)
twListToDF(m2)
m2<-searchTwitter('pyne', geocode='-37.807071,144.939773,50km', n=10)
twListToDF(m2)
library(ggplot2)
str(mpg)
$drv
qplot(displ,hwy,data=mpg)
qplot(displ,year,data=mpg)
qplot(model,cty,data=mpg)
qplot(displ,cty,data=mpg)
qplot(displ,cty,data=mpg, color=drv)
qplot(displ,cty,data=mpg, color=manufacturer)
qplot(displ,cty,data=mpg, color=class)
qplot(displ,cty,data=mpg, color=class, geom=c("point","smooth")
)
qplot(displ,cty,data=mpg, color=class, geom=c("point","smooth"),facets=.~class)
qplot(displ,cty,data=mpg, color=class, geom=c("point","smooth"),facets=.~drv)
qplot(displ,cty,data=mpg, geom=c("point","smooth"),facets=.~drv)
str(mpg)
qplot(displ,data=mpg)
qplot(cty,color=cyl,data=mpg)
qplot(cty,color=drv,data=mpg)
qplot(cty,fill=drv,data=mpg)
qplot(cty,fill=drv,data=mpg,geom="density")
qplot(cty,data=mpg,geom="density")
qplot(cty,data=mpg,geom="density",color=drv)
qplot(cty,data=mpg,geom="density",color=fl)
qplot(cty,data=mpg,geom="density",color=class)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
head(Diet)
str(Bodyweight)
str(nlme)
str(weight)
xyplot(weight ~ Time | Diet, BodyWeight)
library(datasets)
data(airquality)
qplot(Wind,Ozone, data=airquality, facets = .~factor(Month))
library(ggplor2)
library(ggplot2)
qplot(Wind,Ozone, data=airquality, facets = .~factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies, panel = panel.loess)
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + stats_smooth("loess")
g <- ggplot(movies, aes(votes, rating), data =movies)
ggplot(movies, aes(votes, rating)) + geom_point()
statedata <- read.csv("~/Documents/r_code/edx/unit2/statedata.csv")
View(statedata)
setwd("~/Documents/r_code/edx/unit2")
str(statedata)
plot(statedata$x,statedata$y)
?plot
tapply(statedata$HS.Grad,statedata$state.region,mean)
boxplot(statedata$state.region,statedata$Murder)
boxplot(statedata$state.region~statedata$Murder)
?boxplot
boxplot(statedata$Murder ~ statedata$state.region)
which.max(subset(statedata,state.region=="Northeast"))
k=subset(statedata,state.region=="Northeast"))
k=subset(statedata,state.region=="Northeast")
which.max(h)
which.max(h$Murder)
which.max(k$Murder)
k[[6]]
k[6]
k[6,]
k
model = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.GRad + Frost + Area, data = statedata)
model = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(model)
-2.180e-05
-2.180e-05 * 1
plot(statedata$Income,statedata$Life.Exp)
model = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost, data = statedata)
summary(model)
model = lm(Life.Exp ~ Income + Illiteracy + Murder + HS.Grad + Frost, data = statedata)
summary(model)
model = lm(Life.Exp ~ Income+ Murder + HS.Grad + Frost, data = statedata)
summary(model)
model = lm(Life.Exp ~ Income + HS.Grad + Frost + Murder, data = statedata)
summary(model)
model1 = lm(Life.Exp ~ Income + HS.Grad + Frost + Murder, data = statedata)
model2 = lm(Life.Exp ~ HS.Grad + Population +Income + Frost, data = statedata)
model3 = lm(Life.Exp ~ Frost + Murder + HS.Grad + Illiteracy, data = statedata)
model4 = lm(Life.Exp ~ Population + Murder + Frost + HS.Grad, data = statedata)
summary(model1)
summary(model2)
summary(model3)
summary(model4)
model = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(model)
summary(model4)
predict(model4)
?predict
predict(model4,statedata$state)
pp=sort(predict(model4,statedata$state))
pp
which.min(statedata$Life.Exp)
statedata$Life.Exp[40]
statedata$State[40]
statedata$state.name[40]
statedata$state.name[47]
which.max(statedata$Life.Exp)
statedata$state.name[11]
model4$residuals
which.min(model4$residuals)
statedata$state.name[19]
which.max(model4$residuals)
statedata$state.name[11]
which.min(abs(model4$residuals))
statedata$state.name[14]
elantra <- read.csv("~/Documents/r_code/edx/unit2/elantra.csv")
View(elantra)
train = subset(elantra,Year<=2012)
test = subset(elantra, Year > 2012)
model1 = lm(ElantraSales ~ Unemployment + CPI_all + CPI_energy + Queries, data = train)
summary(model)
summary(model1)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + CPI_energy + Queries, data = train)
summary(model2)
predict(model2,data.frame(Month=1,Unemployment=9.7,CPI_energy=213,CPI_all=217,Queries=153))
predict(model2,data.frame(Month=3,Unemployment=9.7,CPI_energy=213,CPI_all=217,Queries=153))
10687.91-10909.28
predict(model2,data.frame(Month=5,Unemployment=9.7,CPI_energy=213,CPI_all=217,Queries=153))
10687.91-11130.65
train$Month = as.factor(train$Month)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + CPI_energy + Queries, data = train)
summary(model2)
cor(train)
train$Month = as.integer(train$Month)
cor(train)
train$Month = as.factor(train$Month)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + CPI_energy + Queries, data = train)
summary(model2)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + Queries, data = train)
summary(model2)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + CPI_energy, data = train)
summary(model2)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + Queries, data = train)
summary(model2)
model2 = lm(ElantraSales ~ Month+ Unemployment + CPI_all + CPI_energy, data = train)
summary(model2)
predict(model2,test)
test$Month = as.factor(test$Month)
predict(model2,test)
er=predict(model2,test)
sum(er^2)
SSE=sum((test$ElantraSales-er)^2)
summary(train$ElantraSales)
SST = sum((test$ElantraSales - mean(train$ElantraSales))^2)
1-SSE/SST
er-test$ElantraSales
sort(abs(er-test$ElantraSales))
test[14,]
View(test)
er
er - test$ElantraSales
